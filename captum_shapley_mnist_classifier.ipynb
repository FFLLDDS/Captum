{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLiftShap (Captum) applied to MNIST classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from captum.attr import DeepLiftShap\n",
    "from captum.attr import visualization as viz\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = 'data'\n",
    "BATCHSIZE = 64\n",
    "LR = 1e-3\n",
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = torchvision.transforms.ToTensor()\n",
    "\n",
    "train_data = torchvision.datasets.MNIST(root=PATH, train=True, transform=transform, download=True)\n",
    "test_data = torchvision.datasets.MNIST(root=PATH, train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCHSIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCHSIZE ,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module): \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.classify = nn.Sequential(\n",
    "                                        nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3), \n",
    "                                        nn.BatchNorm2d(4), \n",
    "                                        nn.ReLU(), \n",
    "                                        # input_size: (BATCHSIZE, 4, 26, 26)\n",
    "                                        nn.Conv2d(in_channels=4, out_channels=16, kernel_size=4, stride=2), \n",
    "                                        nn.BatchNorm2d(16), \n",
    "                                        nn.ReLU(), \n",
    "                                        # input_size: (BATCHSIZE, 16, 12, 12)\n",
    "                                        nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3), \n",
    "                                        nn.ReLU(), \n",
    "                                        # input_size: (BATCHSIZE, 8, 10, 10)\n",
    "                                        nn.Flatten(), \n",
    "                                        nn.Linear(8*10*10, 100), \n",
    "                                        nn.ReLU(), \n",
    "                                        nn.Linear(100, 10), \n",
    "        )\n",
    "        \n",
    "    def forward(self, input): \n",
    "        x = self.classify(input)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Classifier().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(classifier.parameters(), lr=LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "\n",
    "print('Start training classifier...')\n",
    "for epoch in range(NUM_EPOCHS): \n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, batch in enumerate(train_loader): \n",
    "        imgs, labels = batch\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        preds = classifier(imgs)\n",
    "        loss = criterion(preds, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # collect stats\n",
    "        losses.append(loss.item())\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # print stats\n",
    "        if i % 200 == 199:\n",
    "            print(f'[{epoch+1}/{NUM_EPOCHS}] [{i+1}/{len(train_loader)}] Loss classifier: {running_loss / 200}')\n",
    "            running_loss = 0.0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title('training loss')\n",
    "plt.xlabel('batches')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.eval()\n",
    "\n",
    "with torch.no_grad(): \n",
    "        \n",
    "    correct = 0.0\n",
    "    num_test_imgs = 0\n",
    "    \n",
    "    # accuracy\n",
    "    for batch in test_loader: \n",
    "        imgs, labels = batch\n",
    "        \n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        \n",
    "        preds_raw = classifier(imgs)\n",
    "        preds = torch.argmax(preds_raw, dim=1)\n",
    "        \n",
    "        correct += (preds == labels).sum().item() \n",
    "        num_test_imgs += len(labels)\n",
    "        \n",
    "    print(f'The accuracy of the classifier is: {correct / num_test_imgs:.3f}')\n",
    "\n",
    "    # plot some images next to label and prediction\n",
    "    imgs, labels = next(iter(train_loader))\n",
    "    imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "    out_raw = classifier(imgs)\n",
    "    out = torch.argmax(out_raw, dim=1)\n",
    "    \n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    fig.suptitle('(label, prediction)')\n",
    "\n",
    "    for i in range(len(imgs)): \n",
    "        plt.subplot(8, 8, i+1)\n",
    "        plt.axis('off')\n",
    "        plt.imshow(imgs[i].squeeze().detach().cpu().numpy())\n",
    "        plt.title(f'{labels[i].item(), out[i].item()}')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Captum (DeepLiftShap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate DeepLiftShap on model (*see Captum documentation) \n",
    "dl = DeepLiftShap(classifier)\n",
    "\n",
    "# random integer for picking an image\n",
    "num = torch.randint(BATCHSIZE, (1,)).item()\n",
    "\n",
    "# load batch for image and background\n",
    "imgs, labels = next(iter(test_loader))\n",
    "img, label = imgs[num].unsqueeze(dim=1).to(device), labels[0].to(device)\n",
    "\n",
    "background = imgs.to(device)\n",
    "\n",
    "# check shapes\n",
    "print(img.shape)\n",
    "print(background.shape)\n",
    "\n",
    "# predict image class (-> torch.Size([1, 10]))\n",
    "outputs = classifier(img)\n",
    "\n",
    "print(f'Original Image label: {labels[num].item()}')\n",
    "print(f'Predicted: {torch.argmax(outputs).item()}, Probability: {torch.max(torch.nn.functional.softmax(outputs, 1)).item():.2f}')\n",
    "\n",
    "# show image\n",
    "orig_image = imgs[num].squeeze().cpu().detach().numpy()\n",
    "plt.imshow(orig_image)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# calculate attributions of pixels of input image for label (*)\n",
    "attribution = dl.attribute(img, target=label, baselines=background) \n",
    "attribution = np.transpose(attribution.squeeze(0).cpu().detach().numpy(), (1, 2, 0))\n",
    "\n",
    "im = img.squeeze().unsqueeze(dim=0).cpu().detach().numpy() / 2 + 0.5\n",
    "original_image = np.transpose(im, (1, 2, 0))\n",
    "\n",
    "# visualization (*)\n",
    "_ = viz.visualize_image_attr(attribution, original_image, method=\"blended_heat_map\",sign=\"all\", alpha_overlay=0.8, show_colorbar=True, \n",
    "                          title=\"Overlayed DeepLift\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display image next to attribution values of prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DeepLiftShap(classifier)\n",
    "\n",
    "num = torch.randint(BATCHSIZE, (1,)).item()\n",
    "\n",
    "imgs, labels = next(iter(test_loader))\n",
    "background = imgs.to(device)\n",
    "\n",
    "img, label = imgs[num].unsqueeze(dim=1).to(device), labels[num].to(device)\n",
    "outputs = classifier(img)\n",
    "  \n",
    "attribution = dl.attribute(img, target=torch.argmax(outputs).item(), baselines=background) \n",
    "attribution = np.transpose(attribution.squeeze(0).cpu().detach().numpy(), (1, 2, 0))\n",
    "\n",
    "im = img.squeeze().unsqueeze(dim=0).cpu().detach().numpy()\n",
    "original_image = np.transpose(im, (1, 2, 0))\n",
    "_ = viz.visualize_image_attr_multiple(attribution, original_image, methods=[\"original_image\", \"blended_heat_map\"], signs=[\"all\", \"all\"], titles=[f\"label: {label}\", f\"predicted:{torch.argmax(outputs).item()}\"], alpha_overlay=0.8, show_colorbar=True, \n",
    "                            fig_size=(6, 8), use_pyplot=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## display attributions for all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DeepLiftShap(classifier)\n",
    "\n",
    "num = torch.randint(BATCHSIZE, (1,)).item()\n",
    "\n",
    "imgs, labels = next(iter(test_loader))\n",
    "background = imgs.to(device)\n",
    "\n",
    "img, label = imgs[num].unsqueeze(dim=1).to(device), labels[0].to(device)\n",
    "im = img.squeeze().cpu().detach().numpy()\n",
    "\n",
    "fig = plt.figure(figsize=(15,3), tight_layout=True)\n",
    "st = fig.suptitle(\"DeepLiftShap\", fontsize=\"x-large\")\n",
    "ax = plt.subplot(1, 11, 1)\n",
    "ax.axis('off')\n",
    "ax.set_title(f'predicted: {torch.argmax(classifier(img)).item()}')\n",
    "ax.imshow(im)\n",
    "\n",
    "for i in range(10): \n",
    "    attribution = dl.attribute(img, target=i, baselines=background) \n",
    "    attribution = np.transpose(attribution.squeeze(0).cpu().detach().numpy(), (1, 2, 0))\n",
    "    ax = plt.subplot(1, 11, i+2)\n",
    "    im = img.squeeze().unsqueeze(dim=0).cpu().detach().numpy()\n",
    "    original_image = np.transpose(im, (1, 2, 0))\n",
    "    viz.visualize_image_attr(attribution, original_image, method=\"blended_heat_map\",sign=\"all\", plt_fig_axis=(fig, ax), alpha_overlay=0.8, show_colorbar=True, use_pyplot=False)\n",
    "\n",
    "# put suptitle under plots\n",
    "st.set_y(0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8e38f0c188ce6a35b54fcffb420782e7c865c9fec8ce80c6a870b6cb8f84f9de"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit ('island': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
